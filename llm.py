from langchain_openai.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage

class CoachingAdvisorLLM:
    def __init__(self, api_key: str, model_name: str = "gpt-4", temperature: float = 0.7):
        """
        Initialize the LLM instance with OpenAI API credentials.

        :param api_key: OpenAI API key for authentication.
        :param model_name: The model name to use (default is "gpt-4").
        :param temperature: Controls randomness in output.
        """
        if not api_key:
            raise ValueError("API key is not set. Please set the OPENAI_API_KEY environment variable or pass the key explicitly.")

        self.chat = ChatOpenAI(
            openai_api_key=api_key, 
            model=model_name, 
            temperature=temperature
        )

    def generate_advice(self, feedback: str) -> str:
        """
        Generate actionable coaching advice from feedback using GPT-4.

        :param feedback: Feedback text input by the coach.
        :return: Advice generated by GPT-4.
        """
        # Define the system and human message prompts
        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessage(content="You are a professional coaching advisor. Provide actionable, concise, and constructive feedback based on the input."),
            HumanMessage(content="{feedback}")
        ])

        try:
            # Format the messages using the provided feedback
            formatted_messages = prompt_template.format_messages(feedback=feedback)

            # Get the response from the model
            response = self.chat(formatted_messages)
            return response.content.strip()  # Return clean output
        except Exception as e:
            return f"Error while generating advice: {str(e)}"